{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 2\n",
    "\n",
    "`Benjamín Meneses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- Para esta prueba se utilizará el mismo dataset en todos los ejercicios.\n",
    "- Cada registro del dataset corresponde a una transacción realizada con una tarjeta Bip!.\n",
    "- Las columnas son como se describe a continuación:\n",
    "    - <code>fechahoratrx</code>: Fecha y hora a la en la que se realiza la transacción.\n",
    "    - <code>codigoentidad</code>: Código del operador.\n",
    "    - <code>nombreentidad</code>: Nombre del operador.\n",
    "    - <code>codigositio</code>: Código del lugar en el que se realiza la transacción.\n",
    "    - <code>nombresitio</code>: Nombre del lugar en el que se realiza la transacción.\n",
    "    - <code>nrotarjeta</code>: Hash de la tarjeta Bip!\n",
    "- La ubicación del dataset es <code>s3://bigdata-desafio/transantiago/</code>.\n",
    "- Los datos se encuentran en formato columnar Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>12</td><td>application_1643468364668_0013</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-32-198.ec2.internal:20888/proxy/application_1643468364668_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-41-248.ec2.internal:8042/node/containerlogs/container_1643468364668_0013_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<SparkContext master=yarn appName=livy-session-12>"
     ]
    }
   ],
   "source": [
    "# Iniciamos Spark Application\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerías y comandos necesarios\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Identificando usuarios molestosos (4.6 Puntos)\n",
    "Utilizando el archivo user.json.\n",
    "Desde Yelp están interesados en identificar a aquellos usuarios que se pueden considerar\n",
    "como molestosos. Para ello, tienen la siguiente definición de un usuario molestoso:\n",
    "- Un usuario molestoso es aquél que su promedio de evaluaciones es menor o igual a\n",
    "2, tiene en promedio menos de 100 reviews y tiene cero fans.\n",
    "\n",
    "A partir de esta definición, se le solicita los siguientes puntos:\n",
    "- Identifique en una variable dummy todos los usuarios que se puedan clasificar como\n",
    "molestosos acorde al criterio.\n",
    "- Recodificaciones en el archivo user.json:\n",
    "    - friends, que corresponde a un string con todos los user_id de otros\n",
    "        usuarios j que siguen al usuario i. El objetivo es contar la cantidad de\n",
    "        amigos existentes.\n",
    "    - elite, que corresponde a un string con todos los años en los que el usuario\n",
    "        i fue considerado como un reviewer de elite. El objetivo es contar la cantidad\n",
    "        de años en los cuales se consideró como elite.\n",
    "    - Asegúrese de eliminar los siguientes registros: friends, yelping_since,\n",
    "        name, elite, user_id.\n",
    "        \n",
    "### Requerimientos\n",
    "Todos los objetivos se deben resolver utilizando pyspark.\n",
    "- Genere la medición de usuarios molestos en base a los criterios expuestos. (0.8\n",
    "Puntos).\n",
    "- Divida la muestra en conjuntos de entrenamiento (preservando un 70% de los datos)\n",
    "y validación (preservando un 30% de los datos). (0.4 Puntos)\n",
    "- Entrene tres modelos (LogisticRegression, GBTClassifier y\n",
    "DecisionTreeClassifier) sin modificar hiperparámetros que en base a los\n",
    "atributos disponibles en el archivo user.json, clasifique los usuarios molestosos.\n",
    "(2.2 Puntos)\n",
    "- Reporte cuál es el mejor modelo en base a la métrica AUC. (0.4 Puntos)\n",
    "- Identifique cuales son los principales atributos asociados a un usuario molestoso y\n",
    "repórtelos. (0.8 Puntos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>13</td><td>application_1643468364668_0014</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-32-198.ec2.internal:20888/proxy/application_1643468364668_0014/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-41-248.ec2.internal:8042/node/containerlogs/container_1643468364668_0014_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Ingresamos todos los .parque y anidamos en 1 solo pyspark DataFrame\n",
    "df = spark.read.json('s3://bigdata-desafio/yelp-data/user.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average_stars', 'compliment_cool', 'compliment_cute', 'compliment_funny', 'compliment_hot', 'compliment_list', 'compliment_more', 'compliment_note', 'compliment_photos', 'compliment_plain', 'compliment_profile', 'compliment_writer', 'cool', 'elite', 'fans', 'friends', 'funny', 'name', 'review_count', 'useful', 'user_id', 'yelping_since']"
     ]
    }
   ],
   "source": [
    "# Mostramos las columnas para familiarizarnos con el DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(average_stars=4.03, compliment_cool=1, compliment_cute=0, compliment_funny=1, compliment_hot=2, compliment_list=0, compliment_more=0, compliment_note=1, compliment_photos=0, compliment_plain=1, compliment_profile=0, compliment_writer=2, cool=25, elite='2015,2016,2017', fans=5, friends='c78V-rj8NQcQjOI8KP3UEA, alRMgPcngYSCJ5naFRBz5g, ajcnq75Z5xxkvUSmmJ1bCg, BSMAmp2-wMzCkhTfq9ToNg, jka10dk9ygX76hJG0gfPZQ, dut0e4xvme7QSlesOycHQA, l4l5lBnK356zBua7B-UJ6Q, 0HicMOOs-M_gl2eO-zES4Q, _uI57wL2fLyftrcSFpfSGQ, T4_Qd0YWbC3co6WSMw4vxg, iBRoLWPtWmsI1kdbE9ORSA, xjrUcid6Ymq0DoTJELkYyw, GqadWVzJ6At-vgLzK_SKgA, DvB13VJBmSnbFXBVBsKmDA, vRP9nQkYTeNioDjtxZlVhg, gT0A1iN3eeQ8EMAjJhwQtw, 6yCWjFPtp_AD4x93WAwmnw, 1dKzpNnib-JlViKv8_Gt5g, 3Bv4_JxHXq-gVLOxYMQX0Q, ikQyfu1iViYh8T0us7wiFQ, f1GGltNaB7K5DR1jf3dOmg, tgeFUChlh7v8bZFVl2-hjQ, -9-9oyXlqsMG2he5xIWdLQ, Adj9fBPVJad8vSs-mIP7gw, Ce49RY8CKXVsTifxRYFTsw, M1_7TLi8CbdA89nFLlH4iw, wFsNv-hqbW_F5-IRqfBN6g, 0Q1L7zXHocaUZ2gsG2XJeg, cBFgmOCBdhYa0xoFEAzp_g, VrD_AgiFvzqtlR15vir3SQ, cpE-7HK514Sr5vpSen9CEQ, F1UYelhPFB-zIKlt0ygIZg, CQAL1hvsLMCzuJf9AglsXw, 1KnY1wr15WfEWIRLB9IS6g, QWFQ-kXBiLbid-lm5Jr3dQ, nymT8liFugCrM16lTy0ZfQ, qj69bdd885heDvUPCyHd2Q, DySCZZcgbdrlHgEovk5y9w, lZMJIDuvhT9Dy4KyquLXyA, b_9Gn7wS93AoPZPR0dIJqQ, N07g1IaLh0_6sUjtiSRe4w, YdfPX_7DxSnKvvdCJ57iOw, 8GYryZPD22W7WgQ8kvMkEQ, cpQmAgOWatghp14h1pn1dQ, EnchhymLYMqftCRjqvVWmw, -JdfKhFktE7Zs9BMDFcPeQ, uWhC9eof98zPkvsalgaqJw, eyTlNDDaiPatfe6mheIZ0g, VfHq0o73aKsODvfAhwAQtg, kvD5tICngLAaQDujSFWupA, dXacwEhqi9-3_XT6JeH0Og, NfU0zDaTMEQ4-X9dbQWd9A, cTHWBdjSKbctSUIvWsgFxw, 3IEtCbSDF5t7RkZ20T6s9A, HJJXTrp6UybYyPdQ9DA0JA, JaXogQFVjzGRAeBvzamBHg, NUonfKkjS1iVqnNITtgXZg, D5vaJAYp0sOrGfsj9qvsMA, H27Ecbwwu4FGAlLgICourw, S8HrLmMiE4u8FWYWkNEoTw, Io36Y3xWQcIX9rYvPcYfXQ, J5mcqh8KxYpqjaLBNlwcig, -nTB3_08g06fD0GT8AtDBQ, wMpFA46lihK8oFns_5p65A, RZGFJHeomGJCWp3xcL3ejA, ZoQSzzXoSP1RxOD4Amv9Bg, qzM0EB0SkuuGIFv0adjQAQ, HuM6vvuveken-fPZ7d4olA, H3oukHpGpn9n_mJwSDSQyQ, PkmsJsQ8FIZe8eh8c_u96g, wSByVbwME4MzgkJaFyfvNg, YEVqknoDmrHAoUbHX0nPnA, li3vsK1XAPmeJYAUTYflHQ, MKc8yXi0glbPYt0Qb4PECw, fQPH6W9fksi27gkuUPnFaA, amrCMrDsoRetYFg2kwwdFA, 84dVQ6n6r2ezNaTuc7RkKA, yW9QjWY0olv5-uRKv3t_Kw, 5XJDj7c3eoidfQ3jW18Zgw, txSc6a6pIDctvwyBeu7Aqg, HFbbDCyyqP9xPkUlcxeIdg, hTUv5oh2do6Z3OppPuuiJA, gSqonG9J4fNM-fl_fE71AA, pd9mgTFpBTg5F9x-MsczNg, j3VE22V2GcHiH8UZxfFLfw, NYXlMW-T-3V4Jqr4r-i0Wg, btxgAZedxX8IWhMifA7Xkg, -Hp5mPLiRJNFnyeX5Ygzag, P6-DwVg6-t2JuQwIUEk0iQ, OI2TvxYvZrAodBG_RF53Xw, bHxf_VPKmZur1Bier-6A2A, Et_Sb39cVm81_Xe9HDM8ZQ, 5HwGl2UyYbaRq8aD6YC-fA, ZK228WMcCKLo5thcjD7rdw, iTf8wojwfm0NOi7dOiz3Nw, btYRxQYNJjpecflNHtFH0A, Kgo42FzpW_dXFgDKoewbtg, MNk_1Q_dqOY3xxHZKeO8VQ, AlwD504T9k0m5lkg3k5g6Q', funny=17, name='Rashmi', review_count=95, useful=84, user_id='l6BmjZMeQD3rDxWUbiAiow', yelping_since='2013-10-08 23:11:33')]"
     ]
    }
   ],
   "source": [
    "# Hacemos un head del dataframe para ver cómo vienen los datos\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- average_stars: double (nullable = true)\n",
      " |-- compliment_cool: long (nullable = true)\n",
      " |-- compliment_cute: long (nullable = true)\n",
      " |-- compliment_funny: long (nullable = true)\n",
      " |-- compliment_hot: long (nullable = true)\n",
      " |-- compliment_list: long (nullable = true)\n",
      " |-- compliment_more: long (nullable = true)\n",
      " |-- compliment_note: long (nullable = true)\n",
      " |-- compliment_photos: long (nullable = true)\n",
      " |-- compliment_plain: long (nullable = true)\n",
      " |-- compliment_profile: long (nullable = true)\n",
      " |-- compliment_writer: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- elite: string (nullable = true)\n",
      " |-- fans: long (nullable = true)\n",
      " |-- friends: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- yelping_since: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Mostrarmos el tipo de cada dato de cada columna para ver si es necesario cambiar la tipología de alguna\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gereramos la variable objetivo (molestoso)\n",
    "df = df.withColumn('molestoso',\n",
    "                   when((df['fans'] == 0) & (df['average_stars'] <= 2) & (df['review_count'] < 100),\n",
    "                   1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cambiamos el df para que tenga la variable Elite y Friends de la forma\n",
    "# en la que lo necesitamos (contando cada uno de los casos con len(*) y separamos los casos por \",\"),\n",
    "# luego lo pasamos nuevamente a un DF\n",
    "\n",
    "df = df.rdd.map(lambda row: (row['average_stars'],\n",
    "    row['compliment_cool'],\n",
    "row['compliment_cute'],\n",
    "row['compliment_funny'],\n",
    "row['compliment_hot'],\n",
    "row['compliment_list'],\n",
    "row['compliment_more'],\n",
    "row['compliment_note'],\n",
    "row['compliment_photos'],\n",
    "row['compliment_plain'],\n",
    "row['compliment_profile'],\n",
    "row['compliment_writer'],\n",
    "row['cool'],\n",
    "len(row['elite'].split(',')),\n",
    "row['fans'],\n",
    "len(row['friends'].split(',')),\n",
    "row['funny'],\n",
    "row['review_count'],\n",
    "row['useful'],\n",
    "row['molestoso']                             \n",
    ")).toDF(\n",
    "    ['average_stars','compliment_cool',\n",
    "    'compliment_cute',\n",
    "    'compliment_funny',\n",
    "    'compliment_hot',\n",
    "    'compliment_list',\n",
    "    'compliment_more',\n",
    "    'compliment_note',\n",
    "    'compliment_photos',\n",
    "    'compliment_plain',\n",
    "    'compliment_profile',\n",
    "    'compliment_writer',\n",
    "    'cool',\n",
    "    'elite',\n",
    "    'fans',\n",
    "    'friends',\n",
    "    'funny',\n",
    "    'review_count',\n",
    "    'useful',\n",
    "    'molestoso'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|molestoso|  count|\n",
      "+---------+-------+\n",
      "|        0|1453462|\n",
      "|        1| 183676|\n",
      "+---------+-------+"
     ]
    }
   ],
   "source": [
    "# Medimos la cantidad según usuarios molestoso o no molestoso\n",
    "df.groupBy('molestoso').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos VectorAssembler para poder con él hacer un randomsplit que nos permita separar el dataset\n",
    "# en parte de train y en parte test\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Ahora toca importa las librerías necesarias para generar los algoritmos de clasificación\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombramos el vector objetivo a \"label\" y creamos la lista \"feats\" para enlistar las variables que se usarán para predecir\n",
    "df = df.withColumnRenamed('molestoso', 'label')\n",
    "feats = df.columns\n",
    "feats.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craemos nuestro objeto vec y le indicamos las columnas input, así como también nombramos \"assembled_features\" como el\n",
    "# el listado de columnas que usaremos para predecir (Esto es necesario para usar las librerías de los algoritmos)\n",
    "vec = VectorAssembler(inputCols=feats, outputCol='assembled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos nuestro objeto \"vec\" con la data\n",
    "vec = vec.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label=0, assembled_features=DenseVector([4.03, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 25.0, 3.0, 5.0, 99.0, 17.0, 95.0, 84.0]))]"
     ]
    }
   ],
   "source": [
    "# Seleccionamos y nos damos cuenta de que tenemos separado label como el assembled_features\n",
    "vec = vec.select('label', 'assembled_features')\n",
    "vec.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos train y test para entrenar y validar nuestros modelos.\n",
    "train, test = vec.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos Modelo LogisticRegression\n",
    "logistic_example = LogisticRegression(featuresCol='assembled_features',\n",
    "    labelCol='label',\n",
    "    predictionCol='molestoso_pred')\n",
    "logistic_example = logistic_example.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos Modelo de GradientBoostingClassifier\n",
    "gradient_model = GBTClassifier(featuresCol='assembled_features',\n",
    "    labelCol='label',\n",
    "    predictionCol='molestoso_pred').fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos Modelo DecisionTreeClassifier\n",
    "decision_tree_model = DecisionTreeClassifier(featuresCol='assembled_features',\n",
    "    labelCol='label',\n",
    "    predictionCol='molestoso_pred').fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassificationEvaluator_8789fb5725dd"
     ]
    }
   ],
   "source": [
    "#Importamos la librería necesaria para indicarle a cada modelo que genere la columna molestoso_pred\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "# Generamos nuestro objeto \"evaluator\" para que si queremos predecir con un modelo, este genere una \n",
    "# columna llamada \"molestoso_pred\"\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setRawPredictionCol(\"molestoso_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC para Logistic: 0.9999737918990322\n",
      "AUC para Gradient: 0.9999312599816236\n",
      "AUC para DecisionTree: 0.9912848237343601"
     ]
    }
   ],
   "source": [
    "#Dado que todos los modelos el\n",
    "\n",
    "#generamos un for con los 3 modelos para que nos de el indicar AUC para cada set de validación asociado a cada modelo\n",
    "models = {\n",
    "    'Logistic': logistic_example,\n",
    "    'Gradient': gradient_model,\n",
    "    'DecisionTree': decision_tree_model\n",
    "}\n",
    "for name, model in models.items():\n",
    "    print('AUC para {}: {}'.format(name, evaluator.evaluate(model.transform(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos damos cuenta de que el modelo que mejor performa en este ejercicio es Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_stars -155.78639595108254\n",
      "compliment_cool 0.168527253906581\n",
      "compliment_cute 0.9102058654597347\n",
      "compliment_funny 0.168527253906581\n",
      "compliment_hot 0.17076428850508843\n",
      "compliment_list 0.8382656982122865\n",
      "compliment_more 0.04100669477869027\n",
      "compliment_note -0.2676734606037918\n",
      "compliment_photos 0.08822867606929702\n",
      "compliment_plain 0.020375615093030933\n",
      "compliment_profile -0.5180406407873293\n",
      "compliment_writer -0.7919166428908563\n",
      "cool 0.0674008921699163\n",
      "elite -1829.8814626393062\n",
      "fans -185.83223987282145\n",
      "friends -0.0014663230321997173\n",
      "funny 0.046281684448992194\n",
      "review_count -0.16222552866331286\n",
      "useful -0.017149295761152653"
     ]
    }
   ],
   "source": [
    "# Generamos un for que vaya imprimiendo los coeficientes de la Regresión Logística y nos indique qué variables son las más\n",
    "# importantes para que el modelo prediga si es molestoso o no.\n",
    "for name, logodd in zip(feats, list(logistic_example.coefficients)):\n",
    "    print(name, logodd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los 4 atributos que mas influyen para el modelo son:\n",
    "\n",
    "- elite\n",
    "- fans\n",
    "- average_stars\n",
    "- compliment_cute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Identificando la probabilidad de cierre de un servicio (5.4 Puntos)\n",
    "Utilizando el archivo business.json.\n",
    "Desde Yelp están interesados en predecir la probabilidad de cierre de un servicio en base a\n",
    "los reviews y características de un negocio. Así, la primera iteración del modelo es generar\n",
    "una identificación de qué factores están asociados al cierre.\n",
    "El equipo de desarrollo de Yelp le hace entrega de un archivo llamado\n",
    "recoding_business_schema.py que describe:\n",
    "- Atributos a recodificar.\n",
    "- Atributos a mantener.\n",
    "Este archivo sirve como guía y no implementa la recodificación en el\n",
    "pyspark.sql.dataframe.DataFrame, esto es tarea de usted.\n",
    "De manera adicional, cabe destacar que este archivo no incluye la recodificación del vector\n",
    "objetivo (is_open). Usted deberá recodificarla de manera tal de identificar como 1 aquellos\n",
    "servicios que cerraron y 0 el resto.\n",
    "\n",
    "#### Requerimientos\n",
    "Todos los objetivos se deben resolver utilizando pyspark.\n",
    "- Implemente el esquema de recodificación. (0.8 Puntos)\n",
    "- Genere la recodificación del vector objetivo. (0.8 Puntos)\n",
    "- Divida la muestra en conjuntos de entrenamiento (Preservando un 70% de los datos)\n",
    "y validación (preservando un 30% de los datos). (0.4 Puntos)\n",
    "- Entrene tres modelos (LogisticRegression, GBTClassifier y\n",
    "DecisionTreeClassifier) sin modificar hiperparámetros que en base a los\n",
    "atributos recodificados del archivo business.json, clasifique aquellos servicios\n",
    "cerrados. (2.2 Puntos)\n",
    "- Reporte cuál es el mejor modelo en base a la métrica AUC. (0.4 Puntos)\n",
    "- Identifique cuales son los principales atributos asociados al cierre de un servicio.\n",
    "(0.8 Puntos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los datos\n",
    "df_yelp_business = spark.read.json('s3://bigdata-desafio/yelp-data/business.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|is_open| count|\n",
      "+-------+------+\n",
      "|      0| 34084|\n",
      "|      1|158525|\n",
      "+-------+------+"
     ]
    }
   ],
   "source": [
    "# Vemos la variable objetivo a recodificar\n",
    "df_yelp_business.groupBy('is_open').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Copiamos la recodificación del archivo recording_business_schema.py en un hash y las iteramos\n",
    "refactoring = {\n",
    "    'insurance' : when((col('attributes.AcceptsInsurance') == 'True')\\\n",
    "        | (col('attributes.AcceptsInsurance') == \"\\'True\\'\")\\\n",
    "        | (col('attributes.AcceptsInsurance') == \"u\\'True\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'all_ages_allowed' : when((col('attributes.AgesAllowed') == 'allages')\\\n",
    "        | (col('attributes.AgesAllowed') == \"\\'allages\\'\")\\\n",
    "        | (col('attributes.AgesAllowed') == \"u\\'allages\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'alcohol_consumption' : when((col('attributes.Alcohol') == 'beer_and_wine')\\\n",
    "        | (col('attributes.Alcohol') == \"\\'beer_and_wine\\'\")\\\n",
    "        | (col('attributes.Alcohol') == \"u\\'beer_and_wine\\'\")\\\n",
    "        | (col('attributes.Alcohol') == 'full_bar')\\\n",
    "        | (col('attributes.Alcohol') == \"\\'full_bar\\'\")\\\n",
    "        | (col('attributes.Alcohol') == \"u\\'full_bar\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'bitcoin_friendly' : when((col('attributes.BusinessAcceptsBitcoin') == 'True')\\\n",
    "        | (col('attributes.BusinessAcceptsBitcoin') == True)\\\n",
    "        | (col('attributes.BusinessAcceptsBitcoin') == \"\\'True\\'\")\\\n",
    "        | (col('attributes.BusinessAcceptsBitcoin') == \"u\\'True\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'food_business' : when((col('categories').rlike('Food'))\\\n",
    "        | (col('categories').rlike('Restaurants'))\\\n",
    "        | (col('categories').rlike('Bars')), 1)\\\n",
    "         .otherwise(0),\n",
    "    'finance_business' : when((col('categories').rlike('Banks'))\\\n",
    "        | (col('categories').rlike('Insurance'))\\\n",
    "        | (col('categories').rlike('Finance')), 1)\\\n",
    "         .otherwise(0),\n",
    "    'health_business' : when((col('categories').rlike('Fitness'))\\\n",
    "        | (col('categories').rlike('Hospitals'))\\\n",
    "        | (col('categories').rlike('Health')), 1)\\\n",
    "         .otherwise(0),\n",
    "    'smokers' : when((col('attributes.Smoking') == '\\'yes\\'')\\\n",
    "        | (col('attributes.Smoking') == 'u\\'yes\\'')\\\n",
    "        | (col('attributes.Smoking') == 'yes')\\\n",
    "        | (col('attributes.Smoking') == '\\'outdoor\\'')\\\n",
    "        | (col('attributes.Smoking') == 'u\\'outdoor\\'')\\\n",
    "        | (col('attributes.Smoking') == 'outdoor'), 1)\\\n",
    "         .otherwise(0),\n",
    "    'free_wifi' : when((col('attributes.WiFi') == '\\'free\\'')\\\n",
    "        | (col('attributes.WiFi') == 'u\\'free\\'')\\\n",
    "        | (col('attributes.WiFi') == 'free'), 1)\\\n",
    "         .otherwise(0),\n",
    "    'splurge' : when((col('attributes.RestaurantsPriceRange2') == 3)\\\n",
    "        | (col('attributes.RestaurantsPriceRange2') == 4), 1)\\\n",
    "         .otherwise(0),\n",
    "    'kids_friendly' : when((col('attributes.GoodForKids') == 'True')\\\n",
    "        | (col('attributes.GoodForKids') == True)\\\n",
    "        | (col('attributes.GoodForKids') == \"\\'True\\'\")\\\n",
    "        | (col('attributes.GoodForKids') == \"u\\'True\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'has_tv' : when((col('attributes.HasTV') == 'True')\\\n",
    "        | (col('attributes.HasTV') == True)\\\n",
    "        | (col('attributes.HasTV') == \"\\'True\\'\")\\\n",
    "        | (col('attributes.HasTV') == \"u\\'True\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'dogs_friendly' : when((col('attributes.DogsAllowed') == 'True')\\\n",
    "        | (col('attributes.DogsAllowed') == True)\\\n",
    "        | (col('attributes.DogsAllowed') == \"\\True'\\'\")\\\n",
    "        | (col('attributes.DogsAllowed') == \"u\\'True\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'loud_place' : when((col('attributes.NoiseLevel') == 'loud')\\\n",
    "        | (col('attributes.NoiseLevel') == \"\\'loud\\'\")\\\n",
    "        | (col('attributes.NoiseLevel') == \"u\\'loud\\'\")\\\n",
    "        | (col('attributes.NoiseLevel') == \"very_loud\")\\\n",
    "        | (col('attributes.NoiseLevel') == \"\\'very_loud\\'\")\\\n",
    "        | (col('attributes.NoiseLevel') == \"u\\'very_loud\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'happy_hour' : when((col('attributes.HappyHour') == 'True')\\\n",
    "        | (col('attributes.HappyHour') == True)\\\n",
    "        | (col('attributes.HappyHour') == \"\\'True\\'\")\\\n",
    "        | (col('attributes.HappyHour') == \"u\\'True\\'\"), 1)\\\n",
    "         .otherwise(0),\n",
    "    'is_open_recoded' : when((col('is_open') == 0), 1)\\\n",
    "         .otherwise(0)\n",
    "}\n",
    "for colName, refactor in refactoring.items():\n",
    "    df_yelp_business = df_yelp_business.withColumn(colName, refactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(address='2818 E Camino Acequia Drive', attributes=Row(AcceptsInsurance=None, AgesAllowed=None, Alcohol=None, Ambience=None, BYOB=None, BYOBCorkage=None, BestNights=None, BikeParking=None, BusinessAcceptsBitcoin=None, BusinessAcceptsCreditCards=None, BusinessParking=None, ByAppointmentOnly=None, Caters=None, CoatCheck=None, Corkage=None, DietaryRestrictions=None, DogsAllowed=None, DriveThru=None, GoodForDancing=None, GoodForKids='False', GoodForMeal=None, HairSpecializesIn=None, HappyHour=None, HasTV=None, Music=None, NoiseLevel=None, Open24Hours=None, OutdoorSeating=None, RestaurantsAttire=None, RestaurantsCounterService=None, RestaurantsDelivery=None, RestaurantsGoodForGroups=None, RestaurantsPriceRange2=None, RestaurantsReservations=None, RestaurantsTableService=None, RestaurantsTakeOut=None, Smoking=None, WheelchairAccessible=None, WiFi=None), business_id='1SWheh84yJXfytovILXOAQ', categories='Golf, Active Life', city='Phoenix', hours=None, is_open=0, latitude=33.5221425, longitude=-112.0184807, name='Arizona Biltmore Golf Club', postal_code='85016', review_count=5, stars=3.0, state='AZ', insurance=0, all_ages_allowed=0, alcohol_consumption=0, bitcoin_friendly=0, food_business=0, finance_business=0, health_business=0, smokers=0, free_wifi=0, splurge=0, kids_friendly=0, has_tv=0, dogs_friendly=0, loud_place=0, happy_hour=0, is_open_recoded=1), Row(address='30 Eglinton Avenue W', attributes=Row(AcceptsInsurance=None, AgesAllowed=None, Alcohol=\"u'full_bar'\", Ambience=\"{'romantic': False, 'intimate': False, 'classy': False, 'hipster': False, 'divey': False, 'touristy': False, 'trendy': False, 'upscale': False, 'casual': True}\", BYOB=None, BYOBCorkage=None, BestNights=None, BikeParking='False', BusinessAcceptsBitcoin=None, BusinessAcceptsCreditCards=None, BusinessParking=\"{'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}\", ByAppointmentOnly=None, Caters='True', CoatCheck=None, Corkage=None, DietaryRestrictions=None, DogsAllowed=None, DriveThru=None, GoodForDancing=None, GoodForKids='True', GoodForMeal=\"{'dessert': False, 'latenight': False, 'lunch': True, 'dinner': True, 'brunch': False, 'breakfast': False}\", HairSpecializesIn=None, HappyHour=None, HasTV='False', Music=None, NoiseLevel=\"u'loud'\", Open24Hours=None, OutdoorSeating='False', RestaurantsAttire=\"u'casual'\", RestaurantsCounterService=None, RestaurantsDelivery='False', RestaurantsGoodForGroups='True', RestaurantsPriceRange2='2', RestaurantsReservations='True', RestaurantsTableService='True', RestaurantsTakeOut='True', Smoking=None, WheelchairAccessible=None, WiFi=\"u'no'\"), business_id='QXAEGFB4oINsVuTFxEYKFQ', categories='Specialty Food, Restaurants, Dim Sum, Imported Food, Food, Chinese, Ethnic Food, Seafood', city='Mississauga', hours=Row(Friday='9:0-1:0', Monday='9:0-0:0', Saturday='9:0-1:0', Sunday='9:0-0:0', Thursday='9:0-0:0', Tuesday='9:0-0:0', Wednesday='9:0-0:0'), is_open=1, latitude=43.6054989743, longitude=-79.652288909, name='Emerald Chinese Restaurant', postal_code='L5R 3E7', review_count=128, stars=2.5, state='ON', insurance=0, all_ages_allowed=0, alcohol_consumption=1, bitcoin_friendly=0, food_business=1, finance_business=0, health_business=0, smokers=0, free_wifi=0, splurge=0, kids_friendly=1, has_tv=0, dogs_friendly=0, loud_place=1, happy_hour=0, is_open_recoded=0)]"
     ]
    }
   ],
   "source": [
    "df_yelp_business.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactorizamos quedándonos sólo con las columnas necesarias\n",
    "df_yelp_business = df_yelp_business.rdd\\\n",
    "    .map(lambda row: (\n",
    "        row['is_open_recoded'],\n",
    "        row['review_count'],\n",
    "        row['stars'],\n",
    "        row['insurance'],\n",
    "        row['all_ages_allowed'],\n",
    "        row['alcohol_consumption'],\n",
    "        row['bitcoin_friendly'],\n",
    "        row['food_business'],\n",
    "        row['finance_business'],\n",
    "        row['health_business'],\n",
    "        row['smokers'],\n",
    "        row['free_wifi'],\n",
    "        row['splurge'],\n",
    "        row['kids_friendly'],\n",
    "        row['has_tv'],\n",
    "        row['dogs_friendly'],\n",
    "        row['loud_place'],\n",
    "        row['happy_hour']))\\\n",
    "    .toDF([\n",
    "       'label',\n",
    "       'review_count',\n",
    "       'stars',\n",
    "       'insurance',\n",
    "       'all_ages_allowed',\n",
    "       'alcohol_consumption',\n",
    "       'bitcoin_friendly',\n",
    "       'food_business',\n",
    "       'finance_business',\n",
    "       'health_business',\n",
    "       'smokers',\n",
    "       'free_wifi',\n",
    "       'splurge',\n",
    "       'kids_friendly',\n",
    "       'has_tv',\n",
    "       'dogs_friendly',\n",
    "       'loud_place',\n",
    "       'happy_hour',                                       \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos el espacio de atributos del vector objetivo\n",
    "features = df_yelp_business.columns\n",
    "features.remove('label')\n",
    "# Vectorizamos y generamos las muestras de entrenamiento y testing\n",
    "af = VectorAssembler(inputCols = features, outputCol = 'assembled_features')\n",
    "af = af.transform(df_yelp_business)\n",
    "af = af.select('label', 'assembled_features')\n",
    "train, test = af.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryClassificationEvaluator_adbaa6c71fae"
     ]
    }
   ],
   "source": [
    "# Instanciamos el evaluador para nuestros modelos\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.setRawPredictionCol(\"is_open_recoded_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC de modelo Logistic: 0.5103141146535245"
     ]
    }
   ],
   "source": [
    "# Hacemos El modelo de regresión Logística\n",
    "model_logreg = LogisticRegression(featuresCol = 'assembled_features', labelCol = 'label', predictionCol = 'is_open_recoded_pred')\n",
    "model_logreg = model_logreg.fit(train)\n",
    "print('AUC de modelo {}: {}'.format('Logistic', evaluator.evaluate(model_logreg.transform(test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC de modelo GradientBoostTree: 0.538707826411734"
     ]
    }
   ],
   "source": [
    "# Hacemos El modelo de gradient boosted trees\n",
    "model_gboostclassifier = GBTClassifier(featuresCol = 'assembled_features', labelCol = 'label', predictionCol = 'is_open_recoded_pred')\n",
    "model_gboostclassifier = model_gboostclassifier.fit(train)\n",
    "print('AUC de modelo {}: {}'.format('GradientBoostTree', evaluator.evaluate(model_gboostclassifier.transform(test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC de modelo DecisionTree: 0.5292487091686493"
     ]
    }
   ],
   "source": [
    "# Hacemos El modelo de Decision Trees\n",
    "model_dectreeclassifier = DecisionTreeClassifier(featuresCol = 'assembled_features', labelCol = 'label', predictionCol = 'is_open_recoded_pred')\n",
    "model_dectreeclassifier = model_dectreeclassifier.fit(train)\n",
    "print('AUC de modelo {}: {}'.format('DecisionTree', evaluator.evaluate(model_dectreeclassifier.transform(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el mejor modelo es el de GradientBoostTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La importancia de review_count en el modelo es de 0.2064726695212929 \n",
      "La importancia de stars en el modelo es de 0.18008733291171436 \n",
      "La importancia de food_business en el modelo es de 0.15383179005304842 \n",
      "La importancia de alcohol_consumption en el modelo es de 0.0916557104657742 \n",
      "La importancia de kids_friendly en el modelo es de 0.08510811343631108 \n",
      "La importancia de insurance en el modelo es de 0.07483922751961308 \n",
      "La importancia de happy_hour en el modelo es de 0.03836560982941421 \n",
      "La importancia de splurge en el modelo es de 0.03021781803063885 \n",
      "La importancia de has_tv en el modelo es de 0.028669996422767796 \n",
      "La importancia de finance_business en el modelo es de 0.02555553131716045 \n",
      "La importancia de free_wifi en el modelo es de 0.02072203694780641 \n",
      "La importancia de health_business en el modelo es de 0.017481260214469184 \n",
      "La importancia de loud_place en el modelo es de 0.015924394479092514 \n",
      "La importancia de dogs_friendly en el modelo es de 0.014860260177638678 \n",
      "La importancia de smokers en el modelo es de 0.013083880123259981 \n",
      "La importancia de all_ages_allowed en el modelo es de 0.0019110009155580195 \n",
      "La importancia de bitcoin_friendly en el modelo es de 0.001213367634439864"
     ]
    }
   ],
   "source": [
    "# Verificamos las importancias de cada atributo para el mejor modelo\n",
    "importances = list(zip(features, list(model_gboostclassifier.featureImportances)))\n",
    "importances.sort(reverse=True, key=lambda x: x[1])\n",
    "for columna, importance in importances:\n",
    "    print('La importancia de {0} en el modelo es de {1} '.format(columna, importance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
